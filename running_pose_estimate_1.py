# -*- coding: utf-8 -*-
"""Running_Pose_Estimate_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FPqoj9ICthe6dM56tEHJ4pkce-b9-aGd
"""

# Jovian Commit Essentials
# Please retain and execute this cell without modifying the contents for `jovian.commit` to work
!pip install jovian --upgrade -q
import jovian
jovian.set_project('cricketshot')
jovian.set_colab_id('1ryP24Wsf-sRnWi4WotcZVZBGjYpEaVPH')

"""# Overview
The kernel shows how to use the [tf_pose_estimation](https://github.com/ildoonet/tf-pose-estimation) package in Python on a series of running videos.

## Libraries we need
Install tf_pose and pycocotools
"""

!pip install -qq git+https://www.github.com/ildoonet/tf-pose-estimation

!pip install -qq pycocotools

!pip install jovian

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (8, 8)
plt.rcParams["figure.dpi"] = 125
plt.rcParams["font.size"] = 14
plt.rcParams['font.family'] = ['sans-serif']
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.style.use('ggplot')
sns.set_style("whitegrid", {'axes.grid': False})

# Commented out IPython magic to ensure Python compatibility.
import argparse
import logging
import sys
import time

from tf_pose import common
from tqdm import tqdm
import cv2
import os
import pandas as pd
import numpy as np
from tf_pose.estimator import TfPoseEstimator
from tf_pose.networks import get_graph_path, model_wh
import matplotlib.pyplot as plt
# %matplotlib inline

size = '432x368'
model = 'mobilenet_thin'

w, h = model_wh(size)
if w == 0 or h == 0:
    e = TfPoseEstimator(get_graph_path(model), target_size=(432, 368))
else:
    e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))

cutDir = '../input/cricket/cut/cut/'
cutimages = os.listdir(cutDir)
print(len(cutimages))

cutShots = []
cutFiles=[]
pbar = tqdm(total=len(cutimages))
for img in cutimages:
    image = common.read_imgfile(cutDir+img, None, None)
    humans = e.inference(npimg = image, upsample_size=4.0)
    if len(humans)>0:
        cutShots.append(humans[0])
        cutFiles.append(image)
    pbar.update(1)
pbar.close()
print("Cut Shot Examples: ",len(cutFiles))

sweepDir = '../input/cricket/sweep/sweep/'
sweepimages = os.listdir(sweepDir)
print(len(sweepimages))

sweepShots = []
sweepFiles = []
pbar = tqdm(total=len(sweepimages))
for img in sweepimages:
    image = common.read_imgfile(sweepDir+img, None, None)
    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4.0)
    if len(humans)>0:
        sweepShots.append(humans[0])
        sweepFiles.append(image)
        pbar.update(1)
        
pbar.close()
print("Sweep Examples: ",len(sweepFiles))

driveDir = '../input/cricket/drive/drive/'
driveimages = os.listdir(driveDir)
print(len(driveimages))

driveShots = []
driveFiles = []
pbar = tqdm(total=len(driveimages))
for img in driveimages:
    image = common.read_imgfile(driveDir+img, None, None)
    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4.0)
    if len(humans)>0:
        driveShots.append(humans[0])
        driveFiles.append(image)
    pbar.update(1)
        
pbar.close()
print("Drive Examples: ",len(driveFiles))

def humanToDict(hum):
    resultDict = {}
    parts = hum.body_parts.keys()
    for p in parts:
        resultDict[str(p)+'_x'] = hum.body_parts[p].x
        resultDict[str(p)+'_y'] = hum.body_parts[p].y
        #resultDict[str(p)+'_p'] = hum.body_parts[p].p
    return resultDict

cutList = []
for sh in cutShots:
    cutList.append(humanToDict(sh))
    
cutdf = pd.DataFrame(cutList)
#standHumadf['img'] = standFiles
cutdf.head()

sweepList = []
for jh in sweepShots:
    sweepList.append(humanToDict(jh))

sweepdf = pd.DataFrame(sweepList)
sweepdf.head()

driveList = []
for jh in driveShots:
    driveList.append(humanToDict(jh))

drivedf = pd.DataFrame(driveList)
drivedf.head()

sweepdf.shape, cutdf.shape, drivedf.shape,

sweepdf['pose']=0
cutdf['pose']=1
drivedf['pose']=2

alldata = sweepdf.append(cutdf)
alldata = alldata.append(drivedf)

alldata = alldata.reset_index(drop = True)
alldata.shape

allFiles = sweepFiles + cutFiles + driveFiles
allShots = sweepShots + cutShots + driveShots

def min_max_normalize(df):
    xcols = [c for c in df.columns if 'x' in c]
    xdf = df[xcols]
    xdf = xdf.subtract(xdf.min(axis=1), axis=0)
    xdf = xdf.divide(xdf.max(axis=1), axis=0)
    
    ycols = [c for c in df.columns if 'y' in c]
    ydf = df[ycols]
    ydf = ydf.subtract(ydf.min(axis=1), axis=0)
    ydf = ydf.divide(ydf.max(axis=1), axis=0)
    if 'pose' in df.columns:
        resultdf =  pd.concat( [xdf,ydf, df[['pose']]], axis=1 )
    else:
        resultdf =  pd.concat( [xdf,ydf], axis=1 )
    return resultdf

alldata = min_max_normalize(alldata)

alldata.tail()

from sklearn.ensemble  import RandomForestClassifier as rfc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

treatData = alldata.copy()

treatData = treatData.fillna(-1)
treatData.head()

X = treatData[[c for c in treatData.columns if c != 'pose']]
X['img'] = allFiles
X['human'] = allShots
Y = treatData['pose']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2019, shuffle=True)
#X_train, X_test, y_train, y_test = (X,X,Y,Y)

testImg = X_test.img
testHuman = X_test.human
X_train = np.array(X_train[[c for c in X_train.columns if c not in ['img','human'] ]]) 
X_test = np.array(X_test[[c for c in X_test.columns if c not in ['img','human'] ]]) 

print(X_train.shape)

clf = rfc(max_depth=5, n_estimators=10, random_state=2019)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

for p,img,hum in zip(y_pred,testImg,testHuman):
    fig = plt.figure(figsize = (5,5))
    a = fig.add_subplot(1, 1, 1)
    
    if(p==0):
        t='Sweep'
    elif(p==1):
        t='Cut'
    else:
        t='Drive'
    a.set_title(t,fontsize=20)
    resultImage = TfPoseEstimator.draw_humans(img, [hum], imgcopy=False)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

import jovian
jovian.commit(project='cricketshot_ls')

